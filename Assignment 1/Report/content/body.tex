%
%       Abstract
%
\begin{abstract}
    In this report we estimate the latency of the default OpenMPI
    implementation of two collective blocking operations: broadcast
    and scatter. We vary the number of processes, the message size,    
    the core allocation strategy and the distribution algorithm.
\end{abstract}

\maketitle

\section{Introduction}
    OpenMPI is a widely used library for message passing in parallel
    and distributed computing. It provides a set of collective operations
    that allow processes to communicate with each other in a coordinated
    way. In this report we focus on two of these operations: broadcast
    and scatter. Both operations are blocking, meaning that the root
    process will wait until all the other processes have received the
    message before continuing.


\subsection{Distribution algorithms}
    OpenMPI provides several algorithms for distributing the message
    among the processes. The default algorithm is chosen based on the
    message size and the number of processes. Herafter we list the three
    algorithms we used in our experiments:
    \begin{enumerate}
        \item basic linear
        \item chain
        \item binomial tree
        \item binary tree
    \end{enumerate}
\subsubsection{Basic linear}
    The basic linear algorithm is the simplest of the four. It sends
    the message in a linear way: the root process starts by sending
    the message to the first process, then it sends the message to the
    second process, and so on, until all the processes have received
    the message. The total amount of data transmitted is $(n-1) \cdot m$,
    where $n$ is the number of processes and $m$ is the message size.
    With this algorithm, only one process is in charge of sending the
    message to all the other processes.

\subsubsection{Chain}
    The chain algorithm is similar to the basic linear algorithm, but
    instead of sending the message from the first process to the last
    process, it sends the message from the root process to the next
    process, and then from that process to the next one, and so on.
    This way every node is sending data to another node in a given
    instant of time. This implicate that the root process can start
    working on its own data right after sending the first message to
    the first process. \\
    In the case of the scatter operation, we will see later,
    The size of the message decrease at each step, since every
    process is taking from itself a part of the message and sending
    only the remaining part to the next one. The total number of messages
    is the same, however, because of the decreasing in size of the
    message, the distribution becomes faster at each step. 

\subsubsection{Binomial tree}
    With the binomial tree algorithm, each process communicates
    only with a subset of the other processes. In total, the root
    process will send the message to $\log_2(n)$ processes, where
    $n$ is the number of processes. 
    \begin{enumerate}
        \item[$t_0:$] $P_0$ $\rightarrow$ $P_1$,
        \item[$t_1:$] $P_0$ $\rightarrow$ $P_1$, $P_2$ $\rightarrow$ $P_3$,
        \item[$t_2:$] $P_0$ $\rightarrow$ $P_4$, $P_2$ $\rightarrow$ $P_5$, $P_3$ $\rightarrow$ $P_6$,
        \item[$t_3:$] \dots
    \end{enumerate}
    The total amount of data transmitted is $(n-1) \cdot m$, where
    $m$ is the message size. \\
    In the case of the scatter operation, the number of steps
    remain the same, but the amount of data transmitted
    is reduced at each step. Assume $P_0$ holds an array divided
    into $8$ chunks $[ c_0, \dots, c_7 ]$ and $P_1, \dots, p_7$ are
    the other processes.
    \begin{enumerate}
        \item[$t_0:$] $P_0$ $\xrightarrow{c_4,\dots,c_7}$ $P_1$,
        \item[$t_1:$] $P_0$ $\xrightarrow{c_2,c_3}$ $P_2$, $P_1$ $\xrightarrow{c_6,c_7}$ $P_3$,
        \item[$t_2:$] $P_0$ $\xrightarrow{c_1}$ $P_4$, $P_1$ $\xrightarrow{c_5}$ $P_6$, $P_2$ $\xrightarrow{c_4}$ $P_5$, $P_3$ $\xrightarrow{c_7}$ $P_7$,
    \end{enumerate}

\subsubsection{Binary tree}
    The total data to be sent is divided in multiple pieces, one
    for each node. The root process sends the first two chunks to
    two different nodes and these two to their children, and so on,
    until all the nodes have received the data.This way the total number
    of messages sent is $\log_2(n)$, where $n$ is the number of processes.
    The total amount of data transmitted is $log_2(n) \cdot n \cdot m/2$,
    where $m$ is the message size. The drawback of this algorithm is that clearly
    there is a redudndancy in the data transferred due to the
    overlapping paths of communication that are inherent to the binary
    tree structure. This imply there could be issues with the network
    bandwidth, especially when the number of processes is high. \\
    A possible variation consists in changing the number of children each
    node has, hence changing the base of the logarithm, thus reducing
    the contribution of the logarithm in the formula.
    \\
    In the case of the broadcast operation, we will see later,
    every process needs the same data, therefore the redundancy
    is minimized.

\section{Operations}
    In this section we perform three benchmarks, one for the pure
    latency, one for the broadcast operation and one for the scatter
    operation. Since the last two operations build on top of the
    first one, this will act as a baseline for the other two.

% Latency
\input{content/latency.tex}

% Broadcast
\input{content/broadcast.tex}
    
% Scatter
\input{content/scatter.tex}



\section{Conclusions}



