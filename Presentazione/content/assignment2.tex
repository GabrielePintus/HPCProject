
\section{Assignment 2: \newline \textit{The Mandelbrot Set}}
\begin{frame}[fragile]{Assignment 2}
    Implement a hybrid C code which computes the Mandelbrot set using:
    \begin{itemize}
        \item \textbf{OpenMP}: Open Multi-Processing
        \item \textbf{MPI}: Message Passing Interface
    \end{itemize}
\end{frame}

%
%   Mandelbrot Set
%
\begin{frame}[fragile]{Mandelbrot Set}
    Is the set of all complex numbers $c$ for
    which the sequence defined by $z_{n+1} = z_n^2 + c$ does not diverge.
    \begin{figure}[H]
        \centering
        \includegraphics[
            width=0.45\textwidth,
            clip,
            trim=0 35cm 45cm 35cm
        ]{images/mandelbrot_4096x4096_1-min.png}
    \end{figure}
\end{frame}

%
%   Arithmetic Intensity
%
\begin{frame}[fragile]{Arithmetic Intensity}
    Defined as the ratio of the number of arithmetic operations to
    the number of memory operations. \\
    Arithmetic operations:
    \begin{itemize}
        \item 2 mult and 1 add: \texttt{z.x*z.x+z.y*z.y}
        \item 2 mult, 1 sub and 1 add: \texttt{temp=z.x*z.x-z.y*z.y+c.x}
        \item 2 mult and 1 add: \texttt{z.y=2*z.x*z.y+c.y}
        \item 1 add: \texttt{++n}
    \end{itemize}
    Memory operations:
    \begin{itemize}
        \item 1 write: \texttt{int n = 0} 
        \item 2 writes: \texttt{Complex z = \{0,0\}}
        \item 2 reads: \texttt{z.x*z.x+z.y*z.y}
        \item 3 reads and 1 write: \texttt{temp=z.x*z.x-z.y*z.y+c.x}
        \item 3 reads and 1 write: \texttt{z.y=2*z.x*z.y+c.y}
        \item 1 write: \texttt{z.x=temp}
        \item 1 write: \texttt{++n}
    \end{itemize}
\end{frame}
\begin{frame}[fragile]{Arithmetic Intensity}
    The arithmetic intensity is then
    $$
        \frac{11 \cdot N}{ 10\cdot N + 3} 
        = \frac{11}{10+\frac{3}{N}} \xrightarrow{N \to \infty} \frac{11}{10}
        = 1.1 \text{ flops/byte} > 1
    $$
    Compute-bound when the number of iterations is large. \\
    We are not considering optmizations like:
    \begin{itemize}
        \item Intense use of registers
        \item Vectorization
        \item Cache
        \item Loop unrolling
    \end{itemize}
    Therefore, the AI could be higher.
\end{frame}

%
%   OpenMP
%
\begin{frame}[fragile]{OpenMP}
    General purpose API for parallel programming in C, C++ and Fortran.
    Allow to parallelize loops, sections, tasks and more with minimal effort.
    \begin{lstlisting}[language=C, basicstyle=\fontsize{8}{9}\selectfont\ttfamily]
void mandelbrot_set(uint8_t *image) {
    ...
    #pragma omp parallel for schedule(dynamic, CHUNK_SIZE)
    for (int i = 0; i < total_size; ++i) {
        int x = i / HEIGHT;
        int y = i % HEIGHT;
        Complex c = {X_MIN + x * dx, Y_MIN + y * dy};

        // Compute mandelbrot set in the point
        int iter = mandelbrot(c);
        iter = iter % MAX_ITER;

        // store the value
        image[i] = iter;
    }
}
    \end{lstlisting}
\end{frame}
\begin{frame}[fragile]{OpenMP}
    \begin{itemize}
        \item \texttt{\#pragma omp parallel for}: \\
            \quad parallelizes the loop
        \item \texttt{schedule(dynamic, CHUNK\_SIZE)}: \\
            \quad divides the iterations
            in chunks of size \texttt{CHUNK\_SIZE} and assigns \\
            \quad them to threads
            as they become available.
    \end{itemize}
\end{frame}

%
%   Code Optimizations
%
\begin{frame}[fragile,t]{Code Optimizations}
    The following C function \texttt{mandelbrot} computes the Mandelbrot set
    in a given point $c$.
    \begin{lstlisting}[style=customc, basicstyle=\fontsize{8}{9}\selectfont\ttfamily]
int mandelbrot(const Complex c) {
    int n = 0;
    Complex z = {0, 0};
    while ((z.x * z.x + z.y * z.y) < 4 && n < MAX_ITER){
        double temp = z.x * z.x - z.y * z.y + c.x;
        z.y = 2 * z.x * z.y + c.y;
        z.x = temp;
        ++n;
    }
    return n;
}
    \end{lstlisting}
\end{frame}
\begin{frame}[fragile,t]{Code Optimizations}
    Let's cnosider a couple of optimizations:
    \begin{itemize}
        \item \texttt{const Complex c}: the \texttt{const} keyword
        makes the variable \texttt{c} read-only. However, this also
        enables the compiler to pass a pointer to the variable instead of
        copying it.
        \item Magnitude before than \texttt{MAX\_ITER}: for most of the
        points the magnitude of $z$ will be greater than $2$ before reaching
        the maximum number of iterations. Once the first condition is
        false, we don't need to check the second one.
        \item \texttt{++n}: the pre-increment operator is more efficient
        than the post-increment one since it doesn't need to create a temporary
        variable.
    \end{itemize}
\end{frame}

%
%   Compiler's Optimizations
%
\begin{frame}[fragile,t]{Compiler's Optimizations}
    The \textbf{gcc} compiler has an optimization flag \texttt{-O\{0,1,2,3\}} which
    enables several optimizations. We compare the basic code with the
    level 3 optimized one by giving a look at the assembly code. \\
    \begin{itemize}
        \item \textbf{Less memory accesses}: the optimized code uses more intensively
        the CPU registers, avoiding memory accesses. This help reducing the
        \textit{waiting for contention} time. \\
        $20$ vs $70$ commands for memory accesses, $71\%$ less.
        \item \textbf{Optimized memory}: the optimized code uses the \texttt{.p2align}
        directive to align the memory accesses to the cache line size. This
        reduces the number of cache misses. \\
        \small\textit{There could still be a bit of room for manual optimization.}
        \item \textbf{movapd}: the optimized code uses more often the \texttt{movapd}
        instruction, which can moves two doubles at once.
        \item \textbf{Parallelization}: more wisely, the optimized code uses
        the \texttt{gomp\_parallel\_loop\_nonmonotonic\_dynamic} directive.
        Chunk size can vary at runtime, allowing a better load balancing.
    \end{itemize}
\end{frame}

%
%   Results
%
\begin{frame}[fragile,t]{Results}
    With the \texttt{perf stat} command when can measure several metrics. \\ 
    \begin{table}[H]
        \centering
        \begin{tabular}{cccc}
            \toprule
            \textbf{Metric} & \textbf{Basic} & \textbf{Optimized}\\
            \midrule
            Wall-clock time & 40.53 s & 16.09 s \\
            User time & 316.67 s & 126.71 s \\
            System time & 199.23 ms & 70.34 ms \\
            \hline
            Instructions & 881 billions & 466 billions \\
            Instructions per cycle & 1.11 & 1.29 \\
            \hline
            Cache references & 44.743.584 & 43.485.500 \\
            Cache misses & 0.98\% & 0.34\% \\
            \bottomrule
        \end{tabular}
    \end{table}
    \vfill
    Intel core i7-8550U, 1.8 GHz, 4 cores, 8 threads, 16 GB RAM. \\
    OMP threads: $8$, image size: $2048 \times 2048$.
\end{frame}

%
%   MPI
%
\begin{frame}[fragile,t]{MPI}
    Message Passing Interface is a standard for parallel and distributed
    computing. Interaction between processes must be explicitly defined.
    \textbf{Idea}: we want to distribute the load in a dynamic way as
    the OpenMP code does. \\
    \textbf{Solution}: we define a \texttt{MPI\_CHUNK\_SIZE} and we build
    a \textit{work queue}. Each process will get a chunk from the queue
    and will compute it. A new chunk will be assigned to a process
    as it becomes available.
    \newline
    This is a simple, yet effective, way to implement the dynamic scheduler,
    which allows to better distribute the load among the processes.
\end{frame}
\begin{frame}[fragile,t]{MPI - Work Queue}
    The queue data structure, implemented in the \texttt{sys/queue.h} library,
    is built by the root process.
    \begin{lstlisting}[style=customc, basicstyle=\fontsize{7}{7}\selectfont\ttfamily]
    // Create the work queue
    work_queue = (WorkQueue *)malloc(sizeof(WorkQueue));
    TAILQ_INIT(work_queue);

    // Add work items to the queue
    for (uint32_t i = 0; i < n_chunks; ++i){
        WorkItem *item = (WorkItem *)malloc(sizeof(WorkItem));
        item->start_idx = i * MPI_CHUNK_SIZE;
        item->end_idx = (i + 1) * MPI_CHUNK_SIZE;
        if (item->end_idx > total_size)
            item->end_idx = total_size;
        TAILQ_INSERT_TAIL(work_queue, item, entries);
    }
    \end{lstlisting}
\end{frame}
\begin{frame}[fragile,t]{MPI - Communication}
    We use blocking communication to send the chunks to the processes.
    \begin{itemize}
        \item \texttt{MPI\_Send}: sends a message to a process
        \item \texttt{MPI\_Recv}: receives a message from a process
    \end{itemize}
    A further optimization could be to use non-blocking communication
    to overlap computation and communication and reduce the waiting time.
    The root process sends only $2\times4$ bytes, but receive 
    $$2\times4+\texttt{MPI\_CHUNK\_SIZE}\times\texttt{image\_t}$$
    bytes, where \texttt{image\_t} is either $1$ or $2$ bytes. \\
    Hence, the root process is the bottleneck of the communication.

\end{frame}
\begin{frame}[fragile,t]{MPI - Communication}
    One could decide to use two \enquote{weak} processes, one
    for sending the chunks and one for receiving them. Moreover,
    using non-blocking communication reduce the waiting time. \\
    The worker processes can receive a new chunk immediately after
    finishing the previous one. They don't need to wait for:
    \begin{itemize}
        \item The root process to end its own computation
        \item The root process to copy the previous chunk
        \item The communication to be completed
    \end{itemize}
\end{frame}

%
%   Scaling
%
\begin{frame}[fragile]{Scaling}
    We want to measure the scalability of the code in two ways:
    \begin{itemize}
        \item \textbf{Strong scaling}: we fix the problem size and
        we increase the number of processes. We expect the execution
        time to decrease.
        \item \textbf{Weak scaling}: we increase the problem size
        proportionally to the number of processes. We expect the
        execution time to remain constant.
    \end{itemize}
    In both cases we perform the same test once by fixing the number
    of MPI processes to 1 and varying the number of OpenMP threads,
    and once by fixing the number of OpenMP threads to 1 and varying
    the number of MPI processes.
\end{frame}

%
%   Strong Scaling
%
\begin{frame}[fragile]{Strong Scaling - MPI}
    c
\end{frame}



